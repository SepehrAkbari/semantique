{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe20e993",
   "metadata": {},
   "source": [
    "# Semantique\n",
    "\n",
    "Using natural language understanding and deep learning to predict movie star ratings from free-text reviews.\n",
    "\n",
    "In this project, we develop an NLP-based system that infers a star rating (1–5) from an English-language movie review. We preprocess text data, train a deep learning model using PyTorch, and deploy the model for real-time prediction using a lightweight web app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca866f",
   "metadata": {},
   "source": [
    "**Table of Conetents**\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "    - [Imports & Libraries](#imports--libraries)\n",
    "\n",
    "    - [Data Collection](#data-collection)\n",
    "\n",
    "2. [Preprocessing](#preprocessing)\n",
    "\n",
    "    - [Loading Data](#loading-data)\n",
    "\n",
    "    - [Cleaning the Text](#cleaning-the-text)\n",
    "\n",
    "    - [Building the Vocabulary](#building-the-vocabulary)\n",
    "\n",
    "    - [Encoding and Padding](#encoding-and-padding)\n",
    "\n",
    "    - [Creating DataLoaders](#creating-dataloaders)\n",
    "\n",
    "    - [Loading Saved Vocabulary](#loading-saved-vocabulary)\n",
    "\n",
    "3. [Training](#training)\n",
    "\n",
    "    - [Model Architecture](#model-architecture)\n",
    "\n",
    "    - [Training Configuration](#training-configuration)\n",
    "\n",
    "    - [Training Loop](#training-loop)\n",
    "\n",
    "4. [Inference](#inference)\n",
    "\n",
    "    - [Loading the Model](#loading-the-modell)\n",
    "\n",
    "    - [Testing on unseen data](#testing-on-unseen-data)\n",
    "\n",
    "\n",
    "**NOTE:** *The data was preprocessed seperately and training was done on a GPU enabled machine. The point of this notebook to explain the process, rather than training use. To reproduce the model and run the code, refer to the project's repository, and follow the Usage instructions.*\n",
    "\n",
    "Project Repository: [semantique (github)](https://github.com/SepehrAkbari/semantique/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a503e62",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a53c3f",
   "metadata": {},
   "source": [
    "### Imports & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246478fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed26a0",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "We first download and extract the IMDb Large Movie Review dataset if it does not already exist locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab32c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_imdb():\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    filename = \"aclImdb_v1.tar.gz\"\n",
    "    foldername = \"../data/aclImdb\"\n",
    "\n",
    "    if not os.path.exists(foldername):\n",
    "        print(\"Downloading IMDb dataset...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "        print(\"Done!\")\n",
    "    else:\n",
    "        print(\"IMDb dataset already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c317739",
   "metadata": {},
   "source": [
    "## **Preprocessing**\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "In this step we load the positive and negative movie reviews into memory, extracting the rating from filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997b9c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(path):\n",
    "    reviews = []\n",
    "    for filepath in glob.glob(path + \"/*.txt\"):\n",
    "        with open(filepath, encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "        rating = int(filepath.split(\"_\")[-1].split(\".\")[0])\n",
    "        reviews.append((text, rating))\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79997d2e",
   "metadata": {},
   "source": [
    "### Cleaning the Text\n",
    "\n",
    "We clean the text by lowercasing, removing HTML tags, and stripping punctuation. We use regex tokenization to split the text into words. This is a common preprocessing step in NLP, as it allows us to work with individual words rather than entire sentences. We also remove stop words, which are common words that do not carry much meaning (e.g., \"the\", \"is\", \"and\"). This helps to reduce noise in the data and improve the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d568f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return regex_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d476e",
   "metadata": {},
   "source": [
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d06d115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'loved', 'this', 'movie', 'it', 'was', 'amazing']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(\"I loved this movie! <br> It was amazing!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728902fe",
   "metadata": {},
   "source": [
    "### Building the Vocabulary\n",
    "\n",
    "We build a vocabulary of the most frequent words across the training reviews. We use the top 20,000 most frequent words to limit the size of our vocabulary. This is a common practice as it helps to reduce the dimensionality of the data and improve the performance of our model. We also create a mapping from words to indices, which allows us to convert words into numerical representations that can be used as input to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b42a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(tokenized_data, vocab_size = 20000):\n",
    "    counter = Counter()\n",
    "    for tokens, _ in tokenized_data:\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx+2 for idx, (word, _) in enumerate(counter.most_common(vocab_size))}\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<UNK>\"] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd37ab",
   "metadata": {},
   "source": [
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "babf2660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 2,\n",
       " 'movie': 3,\n",
       " 'i': 4,\n",
       " 'loved': 5,\n",
       " 'was': 6,\n",
       " 'great': 7,\n",
       " '<PAD>': 0,\n",
       " '<UNK>': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokenized = [\n",
    "    ([\"i\", \"loved\", \"the\", \"movie\"], 9),\n",
    "    ([\"the\", \"movie\", \"was\", \"great\"], 8)\n",
    "]\n",
    "\n",
    "build_vocab(example_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b2ade",
   "metadata": {},
   "source": [
    "### Encoding and Padding\n",
    "\n",
    "We encode the tokens into integers based on the vocabulary and pad/truncate each review to a fixed maximum length. This is important as it allows us to create a consistent input size for our model. We use padding to ensure that all reviews are the same length, and truncation to limit the maximum length of reviews. This helps to reduce noise in the data and improve the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed2ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens, vocab):\n",
    "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "def pad_sequence(seq, max_len=300):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0905a6a",
   "metadata": {},
   "source": [
    "Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60266516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [2, 3, 4, 1]\n",
      "Padded: [2, 3, 4, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"the\", \"movie\", \"was\", \"awesome\"]\n",
    "vocab = {\"the\": 2, \"movie\": 3, \"was\": 4, \"<UNK>\": 1}\n",
    "\n",
    "seq = encode(tokens, vocab)\n",
    "print(\"Encoded:\", seq)\n",
    "\n",
    "seq_padded = pad_sequence(seq, max_len=10)\n",
    "print(\"Padded:\", seq_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1bf98",
   "metadata": {},
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "We convert the encoded reviews into PyTorch tensors, organize them into datasets, and wrap them with DataLoaders for batch-wise training and evaluation. This is important as it allows us to efficiently load and process the data in batches, which is essential for training. We also create a DataLoader for the test set, which allows us to evaluate the performance of our model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f42773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_data, test_data, vocab, max_len=300, batch_size=64):\n",
    "    X_train = [pad_sequence(encode(tokens, vocab), max_len) for tokens, _ in train_data]\n",
    "    y_train = [rating for _, rating in train_data]\n",
    "\n",
    "    X_test = [pad_sequence(encode(tokens, vocab), max_len) for tokens, _ in test_data]\n",
    "    y_test = [rating for _, rating in test_data]\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.long)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.long)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25a6bc",
   "metadata": {},
   "source": [
    "After this final step of preprocessing we now have a dataset that follows this structure:\n",
    "\n",
    "```python\n",
    "X_batch = [\n",
    "    [2, 3, 4, 1, 0, ..., 0],    # review 1\n",
    "    [7, 6, 9, 2, 5, ..., 0],    # review 2\n",
    "    ...\n",
    "    [1, 2, 3, 4, 5, ..., 0],  # review n\n",
    "]   # Shape: [n, 300]\n",
    "\n",
    "y_batch = [8.0, 9.0, ..., 6.0]  # Shape: [n]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65606e2a",
   "metadata": {},
   "source": [
    "### Loading Saved Vocabulary\n",
    "\n",
    "We define a utility function to load the vocabulary mapping from a saved JSON file. This llows us to reuse the vocabulary mapping across different runs of the model. We also define a function to convert words to indices using the loaded vocabulary, which allows us to encode new reviews for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed7f6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocab():\n",
    "    checkpoint = torch.load(\"../saved_models/best_model.pt\", map_location=\"cpu\")\n",
    "    vocab = checkpoint[\"vocab\"]\n",
    "\n",
    "    with open(\"../saved_models/vocab.json\", \"w\") as f:\n",
    "        json.dump(vocab, f)\n",
    "\n",
    "def load_vocab(path=\"saved_models/vocab.json\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    return {k: int(v) for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc1d9db",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf891d67",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We define a simple LSTM-based regression model to predict movie ratings from text sequences. An LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) that is well-suited for sequence prediction tasks. It is capable of learning long-term dependencies in the data, which makes it a good choice for NLP tasks. We use an embedding layer to convert the input words into dense vectors, followed by an LSTM layer to process the sequences. Finally, we use a linear layer to output the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cc53098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewRegressor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, dropout=0.3, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out).squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3fc2a",
   "metadata": {},
   "source": [
    "Embedding Layer:\n",
    "\n",
    "- This layer turns each word index into a dense vector of size 128.\n",
    "\n",
    "- Now we have a tensor of shape [64, 300, 128], meaning each review is now a sequence of word embeddings.\n",
    "\n",
    "LSTM Layer:\n",
    "\n",
    "- This embedding sequence is passed into a 2-layer LSTM.\n",
    "\n",
    "- The LSTM processes each word in context, capturing the sequence structure — things like grammar, sentiment buildup, negations, and so on.\n",
    "\n",
    "- It outputs a new sequence of the same length: [64, 300, 128], but now with contextualized vectors.\n",
    "\n",
    "Last Hidden State:\n",
    "\n",
    "- Instead of using all 300 outputs, we only keep the last hidden state.\n",
    "\n",
    "- This gives us one 128-dimensional vector per review, representing the model’s understanding of the entire sequence.\n",
    "\n",
    "Dropout:\n",
    "\n",
    "- We apply dropout to reduce overfitting and help the model generalize better.\n",
    "\n",
    "Fully Connected Layer:\n",
    "\n",
    "- This final dense layer maps the 128-dimensional vector to a single number.\n",
    "\n",
    "- It’s a regression output — a float between 1 and 10 representing the predicted rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93d6f4",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "To configure the training procedure, we first prepare the data by creating training and validation splits. We reserve 10% of the training set for validation to monitor generalization performance during training.\n",
    "\n",
    "The model is instantiated using the `ReviewRegressor` architecture, and the GPU is automatically detected for efficient computation. \n",
    "\n",
    "We use the Adam optimizer with a learning rate of 1e-3, and Mean Squared Error (MSE) is chosen as the loss function to directly regress onto the continuous rating scale.\n",
    "\n",
    "Batch-wise data loading is applied to both training and validation sets to enable mini-batch stochastic optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f072fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_training():\n",
    "    train_loader, test_loader, vocab = prepare_data()\n",
    "\n",
    "    full_train_dataset = train_loader.dataset\n",
    "    val_size = int(0.1 * len(full_train_dataset))\n",
    "    train_size = len(full_train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ReviewRegressor(vocab_size=len(vocab)).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a1842",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The training process optimizes the model using mini-batch gradient descent across multiple epochs. After each epoch, the model's performance is evaluated on a held-out validation set to monitor overfitting.\n",
    "\n",
    "The Mean Squared Error (MSE) loss between the predicted ratings and the ground truth ratings is computed for both training and validation sets. Validation loss is used as the primary metric for model checkpointing.\n",
    "\n",
    "The model is saved whenever an improvement in validation loss is detected. To prevent unnecessary computation and overfitting, early stopping is implemented: if validation loss does not improve for a specified number of consecutive epochs (\"patience\"), training is terminated automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41eeb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config_training()\n",
    "    \n",
    "    epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    save_dir = \"../models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'vocab': vocab,\n",
    "                'params': {\n",
    "                    'embed_dim': 128,\n",
    "                    'hidden_dim': 128,\n",
    "                    'num_layers': 2\n",
    "                }\n",
    "            }, os.path.join(save_dir, \"lstm_model.pt\"))\n",
    "            print(\"model saved\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete. Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a1e9e",
   "metadata": {},
   "source": [
    "For each batch:\n",
    "\n",
    "- We move data to device.\n",
    "\n",
    "- Perform a forward pass to get predictions.\n",
    "\n",
    "- Compute loss between predicted and actual ratings with MSE.\n",
    "\n",
    "- Backpropagate the error.\n",
    "\n",
    "- Update the model weights with the optimizer.\n",
    "\n",
    "- Accumulate the loss for tracking.\n",
    "\n",
    "Here is the training loop output:\n",
    "\n",
    "IMDb dataset already exists.\n",
    "\n",
    "Epoch 1, Train Loss: 13.1095, Val Loss: 12.2072\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 2, Train Loss: 12.1692, Val Loss: 12.2012\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 3, Train Loss: 12.1631, Val Loss: 12.2016\n",
    "\n",
    "Epoch 4, Train Loss: 12.0537, Val Loss: 12.2294\n",
    "\n",
    "Epoch 5, Train Loss: 11.6965, Val Loss: 12.0832\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 6, Train Loss: 10.2653, Val Loss: 10.6756\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 7, Train Loss: 8.8276, Val Loss: 7.7987\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 8, Train Loss: 9.7357, Val Loss: 8.6655\n",
    "\n",
    "Epoch 9, Train Loss: 7.3917, Val Loss: 8.3216\n",
    "\n",
    "Epoch 10, Train Loss: 6.4967, Val Loss: 6.2363\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 11, Train Loss: 4.6747, Val Loss: 6.0862\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 12, Train Loss: 3.9636, Val Loss: 5.0654\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 13, Train Loss: 3.4546, Val Loss: 4.7474\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 14, Train Loss: 3.0516, Val Loss: 4.5928\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 15, Train Loss: 2.7172, Val Loss: 4.6426\n",
    "\n",
    "Epoch 16, Train Loss: 2.5244, Val Loss: 4.6294\n",
    "\n",
    "Epoch 17, Train Loss: 2.3367, Val Loss: 4.6185\n",
    "\n",
    "Epoch 18, Train Loss: 2.1732, Val Loss: 4.8058\n",
    "\n",
    "Epoch 19, Train Loss: 2.0793, Val Loss: 4.6943\n",
    "\n",
    "Epoch 20, Train Loss: 1.9065, Val Loss: 4.3475\n",
    "\n",
    "Best model saved!\n",
    "\n",
    "Epoch 21, Train Loss: 1.7136, Val Loss: 4.5039\n",
    "\n",
    "Epoch 22, Train Loss: 1.6124, Val Loss: 4.6814\n",
    "\n",
    "Epoch 23, Train Loss: 1.4649, Val Loss: 4.5539\n",
    "\n",
    "Epoch 24, Train Loss: 1.3505, Val Loss: 4.5139\n",
    "\n",
    "Epoch 25, Train Loss: 1.3097, Val Loss: 4.7228\n",
    "\n",
    "Epoch 26, Train Loss: 1.1824, Val Loss: 4.7661\n",
    "\n",
    "Epoch 27, Train Loss: 1.0835, Val Loss: 4.7407\n",
    "\n",
    "Epoch 28, Train Loss: 1.0418, Val Loss: 4.7476\n",
    "\n",
    "Epoch 29, Train Loss: 0.9816, Val Loss: 5.2269\n",
    "\n",
    "Epoch 30, Train Loss: 0.9809, Val Loss: 4.8599\n",
    "\n",
    "Early stopping triggered.\n",
    "\n",
    "Training complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5175d",
   "metadata": {},
   "source": [
    "## **Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ee69f",
   "metadata": {},
   "source": [
    "### Loading the Model\n",
    "\n",
    "We load the vocabulary mapping from a saved JSON file. This allows us to reuse the vocabulary mapping across different runs of the model. Using the loaded vocabulary, we recreate the model and load the best weights from the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba6844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewRegressor(\n",
       "  (embedding): Embedding(20002, 128, padding_idx=0)\n",
       "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = load_vocab(\"../models/vocab.json\")\n",
    "\n",
    "model_params = {\"embed_dim\": 128, \"hidden_dim\": 128, \"num_layers\": 2}\n",
    "model = ReviewRegressor(vocab_size=len(vocab), **model_params)\n",
    "\n",
    "checkpoint = torch.load(\"../models/lstm_model.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f4461",
   "metadata": {},
   "source": [
    "### Testing on unseen data\n",
    "\n",
    "To evaluate the model's performance, we write some test cases with unknown ratings, which are not part of the training or validation sets. We use the trained model to predict the ratings for these reviews. The predicted ratings can be evaluated by the context and content of the reviews. This allows us to assess the model's ability to generalize to unseen data and make accurate predictions based on the text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "178b059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: A beautiful masterpiece, visually stunning and emotionally resonant.\n",
      "Predicted Rating: 4.5 out of 5 stars\n",
      "\n",
      "Review: Great acting, I would maybe watch it again.\n",
      "Predicted Rating: 3.5 out of 5 stars\n",
      "\n",
      "Review: The movie was a dull and cliched mess from start to finish.\n",
      "Predicted Rating: 1.0 out of 5 stars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_to_stars(pred_scale):\n",
    "    pred_scale = round(min(max(pred_scale, 1), 10))\n",
    "    return pred_scale / 2\n",
    "\n",
    "sample_reviews = [\n",
    "    \"A beautiful masterpiece, visually stunning and emotionally resonant.\",\n",
    "    \"Great acting, I would maybe watch it again.\",\n",
    "    \"The movie was a dull and cliched mess from start to finish.\"\n",
    "]\n",
    "\n",
    "for review in sample_reviews:\n",
    "    tokens = preprocess(review)\n",
    "    encoded = pad_sequence(encode(tokens, vocab))\n",
    "    x = torch.tensor([encoded])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).item()\n",
    "    \n",
    "    stars = convert_to_stars(pred)\n",
    "    print(f\"Review: {review}\\nPredicted Rating: {stars} out of 5 stars\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
